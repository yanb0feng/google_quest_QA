{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:08:52.476065Z","iopub.execute_input":"2023-09-05T02:08:52.476759Z","iopub.status.idle":"2023-09-05T02:08:52.513493Z","shell.execute_reply.started":"2023-09-05T02:08:52.476715Z","shell.execute_reply":"2023-09-05T02:08:52.512177Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/questqa/model_epoch_2_folder_30_In_30_and_1.pt\n/kaggle/input/questqa/model_epoch_1_folder_30_In_30_and_1.pt\n/kaggle/input/questqa/model_epoch_3_folder_30_In_30_and_1.pt\n/kaggle/input/questqa/model_epoch_5_folder_30_In_30_and_1.pt\n/kaggle/input/questqa/model_epoch_7_folder_30_In_30_and_1.pt\n/kaggle/input/questqa/model_epoch_4_folder_30_In_30_and_1.pt\n/kaggle/input/google-quest-challenge/sample_submission.csv\n/kaggle/input/google-quest-challenge/train.csv\n/kaggle/input/google-quest-challenge/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:08:52.515250Z","iopub.execute_input":"2023-09-05T02:08:52.515580Z","iopub.status.idle":"2023-09-05T02:08:52.520398Z","shell.execute_reply.started":"2023-09-05T02:08:52.515553Z","shell.execute_reply":"2023-09-05T02:08:52.519127Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport tensorflow_hub as hub\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport gc\nimport time\nimport random\nimport os\nimport torch\nfrom scipy.stats import spearmanr\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom math import floor, ceil\nfrom transformers import AdamW,BertForSequenceClassification","metadata":{"_cell_guid":"","_uuid":"","execution":{"iopub.status.busy":"2023-09-05T02:08:52.521584Z","iopub.execute_input":"2023-09-05T02:08:52.521951Z","iopub.status.idle":"2023-09-05T02:09:06.973296Z","shell.execute_reply.started":"2023-09-05T02:08:52.521922Z","shell.execute_reply":"2023-09-05T02:09:06.972273Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"../input/google-quest-challenge/sample_submission.csv\")\ntest = pd.read_csv(\"../input/google-quest-challenge/test.csv\")\ntrain = pd.read_csv(\"../input/google-quest-challenge/train.csv\")\n\nMAX_SEQUENCE_LENGTH = 512","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:09:06.976112Z","iopub.execute_input":"2023-09-05T02:09:06.976555Z","iopub.status.idle":"2023-09-05T02:09:07.381772Z","shell.execute_reply.started":"2023-09-05T02:09:06.976513Z","shell.execute_reply":"2023-09-05T02:09:07.380898Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"TARGET_COLUMNS=[ 'question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', \n       'question_well_written', 'answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:09:07.382856Z","iopub.execute_input":"2023-09-05T02:09:07.383196Z","iopub.status.idle":"2023-09-05T02:09:07.388302Z","shell.execute_reply.started":"2023-09-05T02:09:07.383167Z","shell.execute_reply":"2023-09-05T02:09:07.387364Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\nTARGET_COLUMNS_all=['question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written', 'answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:09:07.389489Z","iopub.execute_input":"2023-09-05T02:09:07.389827Z","iopub.status.idle":"2023-09-05T02:09:07.407638Z","shell.execute_reply.started":"2023-09-05T02:09:07.389799Z","shell.execute_reply":"2023-09-05T02:09:07.406696Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print('train shape =', train.shape)\nprint('test shape =', test.shape)\n\ninput_categories = list(train.columns[[1,2,5]])\nprint('\\noutput categories:\\n\\t', train.columns)\nprint('\\ninput categories:\\n\\t', input_categories)","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:09:07.409111Z","iopub.execute_input":"2023-09-05T02:09:07.409411Z","iopub.status.idle":"2023-09-05T02:09:07.420054Z","shell.execute_reply.started":"2023-09-05T02:09:07.409386Z","shell.execute_reply":"2023-09-05T02:09:07.419076Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"train shape = (6079, 41)\ntest shape = (476, 11)\n\noutput categories:\n\t Index(['qa_id', 'question_title', 'question_body', 'question_user_name',\n       'question_user_page', 'answer', 'answer_user_name', 'answer_user_page',\n       'url', 'category', 'host', 'question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written', 'answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written'],\n      dtype='object')\n\ninput categories:\n\t ['question_title', 'question_body', 'answer']\n","output_type":"stream"}]},{"cell_type":"code","source":"n=test['url'].apply(lambda x:(('ell.stackexchange.com' in x) or ('english.stackexchange.com' in x))).tolist()\nspelling=[]\nfor x in n:\n    if x:\n        spelling.append(0.5)\n    else:\n        spelling.append(0.)","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:09:07.421522Z","iopub.execute_input":"2023-09-05T02:09:07.421830Z","iopub.status.idle":"2023-09-05T02:09:07.441753Z","shell.execute_reply.started":"2023-09-05T02:09:07.421804Z","shell.execute_reply":"2023-09-05T02:09:07.440746Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Modified [inital code](https://www.kaggle.com/akensert/bert-base-tf2-0-now-huggingface-transformer) to RoBERTa input format","metadata":{}},{"cell_type":"code","source":"## credit to https://www.kaggle.com/akensert/bert-base-tf2-0-now-huggingface-transformer\n\n\ndef _get_masks(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\ndef _get_segments(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    first_sep = True\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            if first_sep:\n                first_sep = False \n            else:\n                current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\n\n## modified inital code to RoBERTa format\ndef _get_ids(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [1] * (max_seq_length-len(token_ids))\n    return input_ids\n\n## modified inital code to RoBERTa format\ndef _trim_input(title, question, answer, max_sequence_length, \n                t_max_len=30, q_max_len=238, a_max_len=238):\n\n    t = tokenizer.tokenize(title)\n    q = tokenizer.tokenize(question)\n    a = tokenizer.tokenize(answer)\n    \n    t_len = len(t)\n    q_len = len(q)\n    a_len = len(a)\n\n    if (t_len+q_len+a_len+6) > max_sequence_length:\n        \n        if t_max_len > t_len:\n            t_new_len = t_len\n            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n        else:\n            t_new_len = t_max_len\n      \n        if a_max_len > a_len:\n            a_new_len = a_len \n            q_new_len = q_max_len + (a_max_len - a_len)\n        elif q_max_len > q_len:\n            a_new_len = a_max_len + (q_max_len - q_len)\n            q_new_len = q_len\n        else:\n            a_new_len = a_max_len\n            q_new_len = q_max_len\n            \n            \n        if t_new_len+a_new_len+q_new_len+6 != max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\" \n                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+6)))\n        \n        if t_len > t_new_len:\n            ind1 = floor(t_new_len/2)\n            ind2 = ceil(t_new_len/2)\n            t = t[:ind1]+t[-ind2:]\n        else:\n            t = t[:t_new_len]\n\n        if q_len > q_new_len:\n            ind1 = floor(q_new_len/2)\n            ind2 = ceil(q_new_len/2)\n            q = q[:ind1]+q[-ind2:]\n        else:\n            q = q[:q_new_len]\n\n        if a_len > a_new_len:\n            ind1 = floor(a_new_len/2)\n            ind2 = ceil(a_new_len/2)\n            a = a[:ind1]+a[-ind2:]\n        else:\n            a = a[:a_new_len]\n    \n    return t, q, a\n\ndef _convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    \n    stoken = ['<s>'] + title + ['</s>','</s>'] + question + ['</s>','</s>'] + answer + ['</s>']\n\n    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n    input_masks = _get_masks(stoken, max_sequence_length)\n    input_segments = _get_segments(stoken, max_sequence_length)\n\n    return [input_ids, input_masks, input_segments]\n\ndef compute_input_arays(df, columns, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n\n        t, q, a = _trim_input(t, q, a, max_sequence_length)\n\n        ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [np.asarray(input_ids, dtype=np.int32), \n            np.asarray(input_masks, dtype=np.int32), \n            np.asarray(input_segments, dtype=np.int32)]\n\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:09:07.443396Z","iopub.execute_input":"2023-09-05T02:09:07.443935Z","iopub.status.idle":"2023-09-05T02:09:07.462992Z","shell.execute_reply.started":"2023-09-05T02:09:07.443895Z","shell.execute_reply":"2023-09-05T02:09:07.462023Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:09:07.468107Z","iopub.execute_input":"2023-09-05T02:09:07.468524Z","iopub.status.idle":"2023-09-05T02:09:07.477933Z","shell.execute_reply.started":"2023-09-05T02:09:07.468494Z","shell.execute_reply":"2023-09-05T02:09:07.477216Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class TextDataset(torch.utils.data.TensorDataset):\n\n    def __init__(self, x_train, idxs, targets=None):\n        self.input_ids = x_train[0][idxs]\n        self.input_masks = x_train[1][idxs]\n        self.input_segments = x_train[2][idxs]\n        self.targets = targets[idxs] if targets is not None else np.zeros((x_train[0].shape[0], 30))\n\n    def __getitem__(self, idx):\n#         x_train = self.x_train[idx]\n        input_ids =  self.input_ids[idx]\n        input_masks = self.input_masks[idx]\n        input_segments = self.input_segments[idx]\n\n        target = self.targets[idx]\n\n        return input_ids, input_masks, input_segments, target\n\n    def __len__(self):\n        return len(self.input_ids)\n    \n    def collate_fn(self, batch):\n        token_ids = torch.stack([x[0] for x in batch])\n        seg_ids = torch.stack([x[1] for x in batch])\n    \n        if self.labeled:\n            labels = torch.stack([x[2] for x in batch])\n            return token_ids, seg_ids, labels\n        else:\n            return token_ids, seg_ids","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:09:07.479121Z","iopub.execute_input":"2023-09-05T02:09:07.479837Z","iopub.status.idle":"2023-09-05T02:09:07.489670Z","shell.execute_reply.started":"2023-09-05T02:09:07.479798Z","shell.execute_reply":"2023-09-05T02:09:07.488878Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from transformers import RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer, get_cosine_with_hard_restarts_schedule_with_warmup","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:09:07.491030Z","iopub.execute_input":"2023-09-05T02:09:07.491990Z","iopub.status.idle":"2023-09-05T02:09:07.530171Z","shell.execute_reply.started":"2023-09-05T02:09:07.491958Z","shell.execute_reply":"2023-09-05T02:09:07.529065Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"pretrained_weights = 'roberta-base'\ntokenizer = RobertaTokenizer.from_pretrained(pretrained_weights)","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:09:07.531667Z","iopub.execute_input":"2023-09-05T02:09:07.532678Z","iopub.status.idle":"2023-09-05T02:09:10.080003Z","shell.execute_reply.started":"2023-09-05T02:09:07.532643Z","shell.execute_reply":"2023-09-05T02:09:10.078924Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ed364b208a946268059654e3f2f01f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e31d7af74c304bb28a189e0a01fc2d58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ed901e96abe440c9e935ddede0df136"}},"metadata":{}}]},{"cell_type":"code","source":"import re\n\ndef decontracted(phrase):\n    phrase = re.sub(r\"(W|w)on(\\'|\\')t \", \"will not \", phrase)\n    phrase = re.sub(r\"(C|c)an(\\'|\\')t \", \"can not \", phrase)\n    phrase = re.sub(r\"(Y|y)(\\'|\\')all \", \"you all \", phrase)\n    phrase = re.sub(r\"(Y|y)a(\\'|\\')ll \", \"you all \", phrase)\n    phrase = re.sub(r\"(I|i)(\\'|\\')m \", \"i am \", phrase)\n    phrase = re.sub(r\"(A|a)isn(\\'|\\')t \", \"is not \", phrase)\n    phrase = re.sub(r\"n(\\'|\\')t \", \" not \", phrase)\n    phrase = re.sub(r\"(\\'|\\')re \", \" are \", phrase)\n    phrase = re.sub(r\"(\\'|\\')d \", \" would \", phrase)\n    phrase = re.sub(r\"(\\'|\\')ll \", \" will \", phrase)\n    phrase = re.sub(r\"(\\'|\\')t \", \" not \", phrase)\n    phrase = re.sub(r\"(\\'|\\')ve \", \" have \", phrase)\n    \n    return phrase\n\n\ndef clean_text(x):\n\n    x = str(x)\n    for punct in \"/-'\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n        x = x.replace(punct, '')\n    return x\n\ndef clean_numbers(x):\n\n    x = re.sub('[0-9]{5,}', '12345', x)\n    x = re.sub('[0-9]{4}', '1234', x)\n    x = re.sub('[0-9]{3}', '123', x)\n    x = re.sub('[0-9]{2}', '12', x)\n    return x\n\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]\n\nfrom tqdm import tqdm\ndef preprocess_text(text_data):\n    preprocessed_text = []\n    # tqdm is for printing the status bar\n    for sentance in tqdm(text_data):\n        sent = decontracted(sentance)\n        sent = clean_text(sentance)\n        sent = clean_numbers(sentance)\n        sent = sent.replace('\\\\r', ' ')\n        sent = sent.replace('\\\\n', ' ')\n        sent = sent.replace('\\\\\"', ' ')\n        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n        \n\n        sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n        preprocessed_text.append(sent.lower().strip())\n    return preprocessed_text\n\ntrain['question_title'] = preprocess_text(train['question_title'].values)\ntrain['question_body'] = preprocess_text(train['question_body'].values)\ntrain['answer'] = preprocess_text(train['answer'].values)\ntest['question_title'] = preprocess_text(test['question_title'].values)\ntest['question_body'] = preprocess_text(test['question_body'].values)\ntest['answer'] = preprocess_text(test['answer'].values)","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:09:10.081366Z","iopub.execute_input":"2023-09-05T02:09:10.081686Z","iopub.status.idle":"2023-09-05T02:09:17.993709Z","shell.execute_reply.started":"2023-09-05T02:09:10.081659Z","shell.execute_reply":"2023-09-05T02:09:17.992586Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"100%|██████████| 6079/6079 [00:00<00:00, 16574.19it/s]\n100%|██████████| 6079/6079 [00:03<00:00, 1774.78it/s]\n100%|██████████| 6079/6079 [00:03<00:00, 1748.54it/s]\n100%|██████████| 476/476 [00:00<00:00, 15895.36it/s]\n100%|██████████| 476/476 [00:00<00:00, 1743.95it/s]\n100%|██████████| 476/476 [00:00<00:00, 1648.76it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"x_train = compute_input_arays(train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\ny_train1 = compute_output_arrays(train, TARGET_COLUMNS)\ny_train2 = compute_output_arrays(train, TARGET_COLUMNS_all)\nx_test = compute_input_arays(test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:09:17.995251Z","iopub.execute_input":"2023-09-05T02:09:17.995574Z","iopub.status.idle":"2023-09-05T02:09:33.264521Z","shell.execute_reply.started":"2023-09-05T02:09:17.995547Z","shell.execute_reply":"2023-09-05T02:09:33.263395Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"6079it [00:13, 445.03it/s]\n476it [00:01, 448.74it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"bert_config = RobertaConfig.from_pretrained(pretrained_weights) \nbert_config.output_hidden_states=True","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:09:33.266133Z","iopub.execute_input":"2023-09-05T02:09:33.266594Z","iopub.status.idle":"2023-09-05T02:09:33.484482Z","shell.execute_reply.started":"2023-09-05T02:09:33.266553Z","shell.execute_reply":"2023-09-05T02:09:33.483399Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from transformers.optimization import Adafactor, AdafactorSchedule\nfrom transformers import RobertaModel\n","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:09:33.485945Z","iopub.execute_input":"2023-09-05T02:09:33.487096Z","iopub.status.idle":"2023-09-05T02:09:33.492975Z","shell.execute_reply.started":"2023-09-05T02:09:33.487053Z","shell.execute_reply":"2023-09-05T02:09:33.491857Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import RobertaForSequenceClassification, RobertaConfig, RobertaTokenizer\n\n# Define the custom model\nclass CustomRobertaForClassification(nn.Module):\n    def __init__(self, bert_config, num_labels, dropout=0.1, n_use_layer=4):\n        super(CustomRobertaForClassification, self).__init__()\n        self.roberta = RobertaForSequenceClassification.from_pretrained('roberta-base', config=bert_config)\n        self.dropout = nn.Dropout(dropout)\n        self.n_use_layer=n_use_layer\n        # Create custom dense layers for classification\n        self.dense1 = nn.Sequential(nn.Linear(bert_config.hidden_size * n_use_layer, bert_config.hidden_size * n_use_layer), nn.Dropout(dropout))\n        self.dense2 = nn.Sequential(nn.Linear(bert_config.hidden_size * n_use_layer, bert_config.hidden_size * n_use_layer), nn.Dropout(dropout))\n        self.classifier = nn.Linear(bert_config.hidden_size * n_use_layer, num_labels)\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        outputs = self.roberta(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        #hidden_states = outputs.hidden_states [-5:]  # 例如，获取最后5层的隐藏状态\n    \n    # 拼接这些隐藏状态\n        #pooled_output = torch.cat([hidden_state[:, 0] for hidden_state in hidden_states], dim=1)\n        pooled_output = torch.cat([outputs.hidden_states[-1*i][:,0] for i in range(1, self.n_use_layer+1)], dim=1)\n        \n        pooled_output = self.dropout(pooled_output)\n        \n        # Pass through custom dense layers\n        pooled_output = self.dense1(pooled_output)\n        pooled_output = self.dense2(pooled_output)\n        \n        logits = self.classifier(pooled_output)\n        return logits\n\n# Create an instance of the custom model\nnum_labels = 29  # Number of classes for binary classification\nmodel1 = CustomRobertaForClassification(bert_config, num_labels)","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:09:33.494525Z","iopub.execute_input":"2023-09-05T02:09:33.494833Z","iopub.status.idle":"2023-09-05T02:09:40.982103Z","shell.execute_reply.started":"2023-09-05T02:09:33.494806Z","shell.execute_reply":"2023-09-05T02:09:40.981145Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"179e1c60fd954fa8a27f36ae7bdb8360"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"class CustomRobertaForClassification2(nn.Module):\n    def __init__(self, bert_config, num_labels, dropout=0.1, n_use_layer=4):\n        super(CustomRobertaForClassification2, self).__init__()\n        self.roberta = RobertaForSequenceClassification.from_pretrained('roberta-base', config=bert_config)\n        self.dropout = nn.Dropout(dropout)\n        self.n_use_layer=n_use_layer\n        # Create custom dense layers for classification\n        self.dense1 = nn.Sequential(nn.Linear(bert_config.hidden_size * n_use_layer, bert_config.hidden_size * n_use_layer), nn.Dropout(dropout))\n        self.dense2 = nn.Sequential(nn.Linear(bert_config.hidden_size * n_use_layer,bert_config.hidden_size), nn.Dropout(dropout))\n        self.classifier = nn.Linear(bert_config.hidden_size , num_labels)\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        outputs = self.roberta(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        #hidden_states = outputs.hidden_states [-5:]  # 例如，获取最后5层的隐藏状态\n    \n    # 拼接这些隐藏状态\n        #pooled_output = torch.cat([hidden_state[:, 0] for hidden_state in hidden_states], dim=1)\n        pooled_output = torch.cat([outputs.hidden_states[-1*i][:,0] for i in range(1, self.n_use_layer+1)], dim=1)\n        \n        pooled_output = self.dropout(pooled_output)\n        \n        # Pass through custom dense layers\n        pooled_output = self.dense1(pooled_output)\n        pooled_output = self.dense2(pooled_output)\n        \n        logits = self.classifier(pooled_output)\n        return logits\nmodel2 = CustomRobertaForClassification2(bert_config, num_labels)","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:09:40.983427Z","iopub.execute_input":"2023-09-05T02:09:40.985209Z","iopub.status.idle":"2023-09-05T02:09:42.704253Z","shell.execute_reply.started":"2023-09-05T02:09:40.985163Z","shell.execute_reply":"2023-09-05T02:09:42.703145Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# import torch.nn as nn\n# class QAroberta(RobertaForSequenceClassification):\n#     def __init__(self,bert_config,dropout=0.7):\n#         super(QAroberta, self).__init__(bert_config)  \n#         self.n_use_layer = 4\n#         self.model_pretrain = pre_train\n        \n#         self.dense1 = nn.Sequential(nn.Linear(768*self.n_use_layer, 768*self.n_use_layer)\n#                                    ,nn.Dropout(dropout))\n        \n#         self.dense2 = nn.Sequential(nn.Linear(768*self.n_use_layer, 768*self.n_use_layer)\n#                                    ,nn.Dropout(dropout))\n#         self.classes = nn.Sequential(nn.Linear(768*self.n_use_layer, 29)\n#                                    ,nn.Dropout(dropout))\n#     def forward(self,ids,masks,segs):\n#         outputs=self.model_pretrain(input_ids = ids,\n#                              labels = None,\n#                              attention_mask = masks,\n#                             token_type_ids=segs)\n#         pooled_output = torch.cat([outputs[2][-1*i][:,0] for i in range(1, self.n_use_layer+1)], dim=1)\n#         pooled_output = self.dense1(pooled_output)\n#         pooled_output = self.dense2(pooled_output)\n#         logits = self.classes(pooled_output)\n\n#         outputs = (logits,) + outputs[2:]\n\n#         return outputs\n        ","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:09:42.705777Z","iopub.execute_input":"2023-09-05T02:09:42.706229Z","iopub.status.idle":"2023-09-05T02:09:42.712466Z","shell.execute_reply.started":"2023-09-05T02:09:42.706192Z","shell.execute_reply":"2023-09-05T02:09:42.711292Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"num_labels1 = 29  # Number of classes for binary classification\nmodel11 = CustomRobertaForClassification(bert_config, num_labels1)\nmodel12 = CustomRobertaForClassification2(bert_config, num_labels1)\n\nnum_labels2=30\nmodel21 = CustomRobertaForClassification(bert_config, num_labels2)\nmodel22 = CustomRobertaForClassification2(bert_config, num_labels2)","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:09:42.713989Z","iopub.execute_input":"2023-09-05T02:09:42.714410Z","iopub.status.idle":"2023-09-05T02:09:49.949073Z","shell.execute_reply.started":"2023-09-05T02:09:42.714371Z","shell.execute_reply":"2023-09-05T02:09:49.948138Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(num,model_type):\n    if num==29 and model_type==1:\n        model=model11\n        y_train=y_train1\n    if num==29 and model_type==2:\n        model=model12\n        y_train=y_train1\n    if num==30 and model_type==1:\n        model=model21\n        y_train=y_train2\n    if num==30 and model_type==2:\n        model=model22\n        y_train=y_train2\n    optimizer = Adafactor(model.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n    lr_scheduler = AdafactorSchedule(optimizer) \n    NFOLDS = 8\n    BATCH_SIZE = 12\n    EPOCHS = 15\n    SEED = 7345\n\n    seed_everything(SEED)\n\n    model_list = list()\n\n\n    y_oof = np.zeros((len(train), num))\n    test_pred = np.zeros((len(test), num))\n\n    gradient_accumulation_steps = 1\n    kf = KFold(n_splits=NFOLDS, shuffle=True)\n\n    test_loader = torch.utils.data.DataLoader(TextDataset(x_test, test.index),batch_size=BATCH_SIZE, shuffle=False)\n    print(f'In {num} and {model_type}')\n    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    best_acc=100\n    for i, (train_idx, valid_idx) in enumerate(kf.split(x_train[0])):\n\n\n        print(f'fold {i+1}')\n\n        ## loader\n        train_loader=TextDataset(x_train, train_idx, y_train)\n        train_loader = torch.utils.data.DataLoader(,\n                                                   batch_size=BATCH_SIZE, shuffle=True)\n        val_loader=TextDataset(x_train, valid_idx, y_train)\n        val_loader = torch.utils.data.DataLoader(,\n                                                 batch_size=BATCH_SIZE, shuffle=False)\n\n\n        t_total = len(train_loader)//gradient_accumulation_steps*EPOCHS\n\n\n        model.to(device)\n\n        loss_fn = nn.BCEWithLogitsLoss()\n        #print(f'fold {i+1} optimizer in {num} and {model_type}')\n\n\n        for epoch in range(EPOCHS):  \n            #print(f'fold {i+1} epoch {epoch+1}')\n            start_time = time.time()\n            avg_loss = 0.0\n            model.train()\n            start_time1 = time.time()\n            end_time1 = time.time()\n            #print(f'fold {i+1} epoch {epoch+1} train:')\n            for step, data in enumerate(train_loader):\n                start1 = time.time()\n                #print(f'fold {i+1} epoch {epoch+1} train')\n                # get the inputs\n\n                optimizer.zero_grad()\n                input_ids, input_masks, input_segments, labels = data\n                #net.train()\n                pred = model(input_ids = input_ids.long().to(device),\n                                 token_type_ids = input_segments.to(device),\n                                 attention_mask = input_masks.to(device),\n                                )\n\n\n                loss = loss_fn(pred, labels.to(device))\n\n                avg_loss += loss.item()\n                loss = loss / gradient_accumulation_steps\n                loss.backward()\n\n                #if (step + 1) % 2 == 0:\n\n                    # Calling the step function on an Optimizer makes an update to its parameters\n                optimizer.step()\n                lr_scheduler.step()\n                end1=time.time()\n                if (end1-start1)>=2:print(\"time boom!\")\n\n            avg_val_loss = 0.0\n            #print(f'fold {i+1} epoch {epoch+1} val')\n            valid_preds = np.zeros((len(valid_idx), num))\n            true_label = np.zeros((len(valid_idx), num))\n            model.eval()\n            #print(f'fold {i+1} epoch {epoch+1} val')\n            for j,data in enumerate(val_loader):\n\n                # get the inputs\n                input_ids, input_masks, input_segments, labels = data\n                with torch.no_grad():\n                    pred = model(input_ids = input_ids.long().to(device),\n                                     token_type_ids = input_segments.to(device),\n                                     attention_mask = input_masks.to(device),\n                                    )\n\n                    loss_val = loss_fn(pred, labels.to(device))\n                    avg_val_loss += loss_val.item()\n\n                    pred = torch.sigmoid(pred)\n                valid_preds[j * BATCH_SIZE:(j+1) * BATCH_SIZE] = pred.cpu().detach().numpy()\n                true_label[j * BATCH_SIZE:(j+1) * BATCH_SIZE]  = labels\n\n\n            score = 0\n            for i in range(num):\n              s = np.nan_to_num(\n                        spearmanr(true_label[:, i], valid_preds[:, i]).correlation / num)\n              score += s\n\n\n            elapsed_time = time.time() - start_time \n            print('Epoch {}/{} \\t loss={:.4f}\\t val_loss={:.4f}\\t spearmanr={:.4f}\\t time={:.2f}s'.format(epoch+1, EPOCHS, avg_loss/len(train_loader),avg_val_loss/len(val_loader),score, elapsed_time))\n            val_acc=avg_val_loss/len(val_loader)\n            if val_acc < best_acc and val_acc<0.35:\n                    best_acc = val_acc\n                    model_path = f\"model_epoch_{epoch + 1}_folder_{i+1}_In_{num}_and_{model_type}.pt\"\n                    torch.save(model.state_dict(), model_path)\n                    print('saving model with acc {:.3f}'.format(best_acc))\n\n\n\n        model_list.append(model)\n        y_oof[valid_idx] = valid_preds\n\n\n        result = list()\n        with torch.no_grad():\n            for data in test_loader:\n                input_ids, input_masks, input_segments, labels = data\n                y_pred = model(input_ids = input_ids.long().to(device),\n                                    token_type_ids = input_segments.to(device),\n                                    attention_mask = input_masks.to(device),\n                                )\n\n                y_pred = torch.sigmoid(y_pred)\n                result.extend(y_pred.cpu().detach().numpy())\n\n        test_pred += np.array(result)/NFOLDS\n\n    return test_pred\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:09:49.950670Z","iopub.execute_input":"2023-09-05T02:09:49.951568Z","iopub.status.idle":"2023-09-05T02:09:49.961885Z","shell.execute_reply.started":"2023-09-05T02:09:49.951524Z","shell.execute_reply":"2023-09-05T02:09:49.961018Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def test_output(num,model_type,model_path):\n    if num==29 and model_type==1:\n        model=model11\n        y_train=y_train1\n    if num==29 and model_type==2:\n        model=model12\n        y_train=y_train1\n    if num==30 and model_type==1:\n        model=model21\n        y_train=y_train2\n    if num==30 and model_type==2:\n        model=model22\n        y_train=y_train2\n    NFOLDS = 8\n    BATCH_SIZE = 12\n    EPOCHS = 15\n    SEED = 7345\n    load_params = torch.load(model_path, map_location=torch.device('cpu'))\n    model_params = model.state_dict()\n    same_parsms = {k: v for k, v in load_params.items() if k in model_params.keys()}\n    # 更新模型参数字典，并载入\n    model_params.update(same_parsms)\n    model.load_state_dict(model_params)\n    seed_everything(SEED)\n\n    model_list = list()\n    \n\n    y_oof = np.zeros((len(train), num))\n    test_pred = np.zeros((len(test), num))\n\n    gradient_accumulation_steps = 1\n    kf = KFold(n_splits=NFOLDS, shuffle=True)\n\n    test_loader = torch.utils.data.DataLoader(\n        TextDataset(x_test, test.index),batch_size=BATCH_SIZE, shuffle=False)\n    \n    device=torch.device('cpu')\n    print(f'In {num} and {model_type}')\n    result = list()\n    with torch.no_grad():\n        for data in test_loader:\n            input_ids, input_masks, input_segments, labels = data\n            y_pred = model(input_ids = input_ids.long().to(device),\n                                    token_type_ids = input_segments.to(device),\n                                    attention_mask = input_masks.to(device),\n                                )\n\n            y_pred = torch.sigmoid(y_pred)\n            result.extend(y_pred.cpu().detach().numpy())\n        \n    test_pred = np.array(result)\n\n    return test_pred\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:09:49.963693Z","iopub.execute_input":"2023-09-05T02:09:49.964193Z","iopub.status.idle":"2023-09-05T02:09:49.978884Z","shell.execute_reply.started":"2023-09-05T02:09:49.964154Z","shell.execute_reply":"2023-09-05T02:09:49.977911Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n    \ndef postprocessing(oof_df):\n   \n    scaler = MinMaxScaler()\n    \n    # type 1 column [0, 0.333333, 0.5, 0.666667, 1]\n    # type 2 column [0, 0.333333, 0.666667]\n    # type 3 column [0.333333, 0.444444, 0.5, 0.555556, 0.666667, 0.777778, 0.8333333, 0.888889, 1]\n    # type 4 column [0.200000, 0.266667, 0.300000, 0.333333, 0.400000, \\\n    # 0.466667, 0.5, 0.533333, 0.600000, 0.666667, 0.700000, \\\n    # 0.733333, 0.800000, 0.866667, 0.900000, 0.933333, 1]\n    \n    # comment some columns based on oof result\n    \n    ################################################# handle type 1 columns\n    type_one_column_list = [\n       'question_conversational', \\\n       'question_has_commonly_accepted_answer', \\\n       'question_not_really_a_question', \\\n       'question_type_choice', \\\n       'question_type_compare', \\\n       'question_type_consequence', \\\n       'question_type_definition', \\\n       'question_type_entity', \\\n       'question_type_instructions', \n    ]\n    \n    oof_df[type_one_column_list] = scaler.fit_transform(oof_df[type_one_column_list])\n    \n    tmp = oof_df.copy(deep=True)\n    \n    for column in type_one_column_list:\n        \n        oof_df.loc[tmp[column] <= 0.16667, column] = 0\n        oof_df.loc[(tmp[column] > 0.16667) & (tmp[column] <= 0.41667), column] = 0.333333\n        oof_df.loc[(tmp[column] > 0.41667) & (tmp[column] <= 0.58333), column] = 0.500000\n        oof_df.loc[(tmp[column] > 0.58333) & (tmp[column] <= 0.73333), column] = 0.666667\n        oof_df.loc[(tmp[column] > 0.73333), column] = 1\n    \n    \n    \n    ################################################# handle type 2 columns      \n#     type_two_column_list = [\n#         'question_type_spelling'\n#     ]\n    \n#     for column in type_two_column_list:\n#         if sum(tmp[column] > 0.15)>0:\n#             oof_df.loc[tmp[column] <= 0.15, column] = 0\n#             oof_df.loc[(tmp[column] > 0.15) & (tmp[column] <= 0.45), column] = 0.333333\n#             oof_df.loc[(tmp[column] > 0.45), column] = 0.666667\n#         else:\n#             t1 = max(int(len(tmp[column])*0.0013),2)\n#             t2 = max(int(len(tmp[column])*0.0008),1)\n#             thred1 = sorted(list(tmp[column]))[-t1]\n#             thred2 = sorted(list(tmp[column]))[-t2]\n#             oof_df.loc[tmp[column] <= thred1, column] = 0\n#             oof_df.loc[(tmp[column] > thred1) & (tmp[column] <= thred2), column] = 0.333333\n#             oof_df.loc[(tmp[column] > thred2), column] = 0.666667\n    \n    \n    \n    ################################################# handle type 3 columns      \n    type_three_column_list = [\n       'question_interestingness_self', \n    ]\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    oof_df[type_three_column_list] = scaler.fit_transform(oof_df[type_three_column_list])\n    tmp[type_three_column_list] = scaler.fit_transform(tmp[type_three_column_list])\n    \n    for column in type_three_column_list:\n        oof_df.loc[tmp[column] <= 0.385, column] = 0.333333\n        oof_df.loc[(tmp[column] > 0.385) & (tmp[column] <= 0.47), column] = 0.444444\n        oof_df.loc[(tmp[column] > 0.47) & (tmp[column] <= 0.525), column] = 0.5\n        oof_df.loc[(tmp[column] > 0.525) & (tmp[column] <= 0.605), column] = 0.555556\n        oof_df.loc[(tmp[column] > 0.605) & (tmp[column] <= 0.715), column] = 0.666667\n        oof_df.loc[(tmp[column] > 0.715) & (tmp[column] <= 0.8), column] = 0.833333\n        oof_df.loc[(tmp[column] > 0.8) & (tmp[column] <= 0.94), column] = 0.888889\n        oof_df.loc[(tmp[column] > 0.94), column] = 1\n        \n        \n        \n    ################################################# handle type 4 columns      \n    type_four_column_list = [\n        'answer_satisfaction'\n    ]\n    scaler = MinMaxScaler(feature_range=(0.2, 1))\n    oof_df[type_four_column_list] = scaler.fit_transform(oof_df[type_four_column_list])\n    tmp[type_four_column_list] = scaler.fit_transform(tmp[type_four_column_list])\n    \n    for column in type_four_column_list:\n        \n        oof_df.loc[tmp[column] <= 0.233, column] = 0.200000\n        oof_df.loc[(tmp[column] > 0.233) & (tmp[column] <= 0.283), column] = 0.266667\n        oof_df.loc[(tmp[column] > 0.283) & (tmp[column] <= 0.315), column] = 0.300000\n        oof_df.loc[(tmp[column] > 0.315) & (tmp[column] <= 0.365), column] = 0.333333\n        oof_df.loc[(tmp[column] > 0.365) & (tmp[column] <= 0.433), column] = 0.400000\n        oof_df.loc[(tmp[column] > 0.433) & (tmp[column] <= 0.483), column] = 0.466667\n        oof_df.loc[(tmp[column] > 0.483) & (tmp[column] <= 0.517), column] = 0.500000\n        oof_df.loc[(tmp[column] > 0.517) & (tmp[column] <= 0.567), column] = 0.533333\n        oof_df.loc[(tmp[column] > 0.567) & (tmp[column] <= 0.633), column] = 0.600000\n        oof_df.loc[(tmp[column] > 0.633) & (tmp[column] <= 0.683), column] = 0.666667\n        oof_df.loc[(tmp[column] > 0.683) & (tmp[column] <= 0.715), column] = 0.700000\n        oof_df.loc[(tmp[column] > 0.715) & (tmp[column] <= 0.767), column] = 0.733333\n        oof_df.loc[(tmp[column] > 0.767) & (tmp[column] <= 0.833), column] = 0.800000\n        oof_df.loc[(tmp[column] > 0.883) & (tmp[column] <= 0.915), column] = 0.900000\n        oof_df.loc[(tmp[column] > 0.915) & (tmp[column] <= 0.967), column] = 0.933333\n        oof_df.loc[(tmp[column] > 0.967), column] = 1\n    \n    \n    ################################################# round to i / 90 (i from 0 to 90)\n    oof_values = oof_df[TARGET_COLUMNS].values\n    DEGREE = len(oof_df)//45*9\n#     if degree:\n#         DEGREE = degree\n#     DEGREE = 90\n    oof_values = np.around(oof_values * DEGREE) / DEGREE  ### 90 To be changed\n    oof_df[TARGET_COLUMNS] = oof_values\n    \n    return oof_df","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:09:49.980437Z","iopub.execute_input":"2023-09-05T02:09:49.981236Z","iopub.status.idle":"2023-09-05T02:09:49.997486Z","shell.execute_reply.started":"2023-09-05T02:09:49.981197Z","shell.execute_reply":"2023-09-05T02:09:49.996667Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"test_pred11=test_output(30,1,'../input/questqa/model_epoch_7_folder_30_In_30_and_1.pt')\n#test_pred12=train_model(29,2)\n#test_pred21=train_model(30,1)\n#test_pred22=train_model(30,2)\n# test_pred11.to_csv('test_pred11.csv', index=False)\n# test_pred12.to_csv('test_pred12.csv', index=False)\n# test_pred21.to_csv('test_pred21.csv', index=False)\n# test_pred22.to_csv('test_pred22.csv', index=False)\n\n# sample_submission.loc[:, TARGET_COLUMNS] = test_pred\n\n# #sample_submission = postprocessing(sample_submission)\n# sample_submission[ sample_submission[TARGET_COLUMNS] > 1.0] = 1.0\n# sample_submission['question_type_spelling'] = spelling","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:09:49.998942Z","iopub.execute_input":"2023-09-05T02:09:49.999809Z","iopub.status.idle":"2023-09-05T02:19:45.219788Z","shell.execute_reply.started":"2023-09-05T02:09:49.999767Z","shell.execute_reply":"2023-09-05T02:19:45.218485Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"In 30 and 1\n","output_type":"stream"}]},{"cell_type":"code","source":"# sample_submission.loc[:, TARGET_COLUMNS] = test_pred11\n# sample_submission['question_type_spelling'] = spelling\n# sample_submission.to_csv('submission11.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:19:45.221396Z","iopub.execute_input":"2023-09-05T02:19:45.221759Z","iopub.status.idle":"2023-09-05T02:19:45.226229Z","shell.execute_reply.started":"2023-09-05T02:19:45.221726Z","shell.execute_reply":"2023-09-05T02:19:45.225063Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# sample_submission.loc[:, TARGET_COLUMNS] = test_pred12\n# sample_submission['question_type_spelling'] = spelling\n# sample_submission.to_csv('submission12.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:19:45.228772Z","iopub.execute_input":"2023-09-05T02:19:45.229239Z","iopub.status.idle":"2023-09-05T02:19:45.244842Z","shell.execute_reply.started":"2023-09-05T02:19:45.229204Z","shell.execute_reply":"2023-09-05T02:19:45.244011Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"sample_submission.loc[:, TARGET_COLUMNS_all] = test_pred11\nsample_submission.to_csv('submission21.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:35:25.091548Z","iopub.execute_input":"2023-09-05T02:35:25.092055Z","iopub.status.idle":"2023-09-05T02:35:25.143072Z","shell.execute_reply.started":"2023-09-05T02:35:25.092019Z","shell.execute_reply":"2023-09-05T02:35:25.141638Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# sample_submission.loc[:, TARGET_COLUMNS_all] = test_pred22\n# sample_submission.to_csv('submission22.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-05T02:19:45.947808Z","iopub.status.idle":"2023-09-05T02:19:45.948674Z","shell.execute_reply.started":"2023-09-05T02:19:45.948366Z","shell.execute_reply":"2023-09-05T02:19:45.948395Z"},"trusted":true},"execution_count":null,"outputs":[]}]}