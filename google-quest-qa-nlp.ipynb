{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:24:35.046871Z","iopub.execute_input":"2023-08-23T14:24:35.047332Z","iopub.status.idle":"2023-08-23T14:24:35.062692Z","shell.execute_reply.started":"2023-08-23T14:24:35.047292Z","shell.execute_reply":"2023-08-23T14:24:35.061694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:24:35.064162Z","iopub.execute_input":"2023-08-23T14:24:35.065275Z","iopub.status.idle":"2023-08-23T14:24:35.077762Z","shell.execute_reply.started":"2023-08-23T14:24:35.065238Z","shell.execute_reply":"2023-08-23T14:24:35.076511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:24:35.081537Z","iopub.execute_input":"2023-08-23T14:24:35.081794Z","iopub.status.idle":"2023-08-23T14:24:48.183587Z","shell.execute_reply.started":"2023-08-23T14:24:35.081770Z","shell.execute_reply":"2023-08-23T14:24:48.182367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport tensorflow_hub as hub\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport gc\nimport time\nimport random\nimport os\nimport torch\nfrom scipy.stats import spearmanr\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom math import floor, ceil\nfrom transformers import AdamW,BertForSequenceClassification","metadata":{"_cell_guid":"","_uuid":"","execution":{"iopub.status.busy":"2023-08-23T14:24:48.188587Z","iopub.execute_input":"2023-08-23T14:24:48.189591Z","iopub.status.idle":"2023-08-23T14:24:54.822055Z","shell.execute_reply.started":"2023-08-23T14:24:48.189550Z","shell.execute_reply":"2023-08-23T14:24:54.821099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"../input/google-quest-challenge/sample_submission.csv\")\ntest = pd.read_csv(\"../input/google-quest-challenge/test.csv\")\ntrain = pd.read_csv(\"../input/google-quest-challenge/train.csv\")\n\nMAX_SEQUENCE_LENGTH = 512","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:24:54.823514Z","iopub.execute_input":"2023-08-23T14:24:54.823847Z","iopub.status.idle":"2023-08-23T14:24:55.265021Z","shell.execute_reply.started":"2023-08-23T14:24:54.823812Z","shell.execute_reply":"2023-08-23T14:24:55.263995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TARGET_COLUMNS=[ 'question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', \n       'question_well_written', 'answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:24:55.266430Z","iopub.execute_input":"2023-08-23T14:24:55.266880Z","iopub.status.idle":"2023-08-23T14:24:55.275659Z","shell.execute_reply.started":"2023-08-23T14:24:55.266839Z","shell.execute_reply":"2023-08-23T14:24:55.274065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('train shape =', train.shape)\nprint('test shape =', test.shape)\n\ninput_categories = list(train.columns[[1,2,5]])\nprint('\\noutput categories:\\n\\t', TARGET_COLUMNS)\nprint('\\ninput categories:\\n\\t', input_categories)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:24:55.277458Z","iopub.execute_input":"2023-08-23T14:24:55.278232Z","iopub.status.idle":"2023-08-23T14:24:55.292386Z","shell.execute_reply.started":"2023-08-23T14:24:55.278196Z","shell.execute_reply":"2023-08-23T14:24:55.290121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n=test['url'].apply(lambda x:(('ell.stackexchange.com' in x) or ('english.stackexchange.com' in x))).tolist()\nspelling=[]\nfor x in n:\n    if x:\n        spelling.append(0.5)\n    else:\n        spelling.append(0.)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:24:55.294099Z","iopub.execute_input":"2023-08-23T14:24:55.294607Z","iopub.status.idle":"2023-08-23T14:24:55.305703Z","shell.execute_reply.started":"2023-08-23T14:24:55.294564Z","shell.execute_reply":"2023-08-23T14:24:55.304527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modified [inital code](https://www.kaggle.com/akensert/bert-base-tf2-0-now-huggingface-transformer) to RoBERTa input format","metadata":{}},{"cell_type":"code","source":"## credit to https://www.kaggle.com/akensert/bert-base-tf2-0-now-huggingface-transformer\n\n\ndef _get_masks(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\ndef _get_segments(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    first_sep = True\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            if first_sep:\n                first_sep = False \n            else:\n                current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\n\n## modified inital code to RoBERTa format\ndef _get_ids(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [1] * (max_seq_length-len(token_ids))\n    return input_ids\n\n## modified inital code to RoBERTa format\ndef _trim_input(title, question, answer, max_sequence_length, \n                t_max_len=30, q_max_len=238, a_max_len=238):\n\n    t = tokenizer.tokenize(title)\n    q = tokenizer.tokenize(question)\n    a = tokenizer.tokenize(answer)\n    \n    t_len = len(t)\n    q_len = len(q)\n    a_len = len(a)\n\n    if (t_len+q_len+a_len+6) > max_sequence_length:\n        \n        if t_max_len > t_len:\n            t_new_len = t_len\n            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n        else:\n            t_new_len = t_max_len\n      \n        if a_max_len > a_len:\n            a_new_len = a_len \n            q_new_len = q_max_len + (a_max_len - a_len)\n        elif q_max_len > q_len:\n            a_new_len = a_max_len + (q_max_len - q_len)\n            q_new_len = q_len\n        else:\n            a_new_len = a_max_len\n            q_new_len = q_max_len\n            \n            \n        if t_new_len+a_new_len+q_new_len+6 != max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\" \n                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+6)))\n        \n        if t_len > t_new_len:\n            ind1 = floor(t_new_len/2)\n            ind2 = ceil(t_new_len/2)\n            t = t[:ind1]+t[-ind2:]\n        else:\n            t = t[:t_new_len]\n\n        if q_len > q_new_len:\n            ind1 = floor(q_new_len/2)\n            ind2 = ceil(q_new_len/2)\n            q = q[:ind1]+q[-ind2:]\n        else:\n            q = q[:q_new_len]\n\n        if a_len > a_new_len:\n            ind1 = floor(a_new_len/2)\n            ind2 = ceil(a_new_len/2)\n            a = a[:ind1]+a[-ind2:]\n        else:\n            a = a[:a_new_len]\n    \n    return t, q, a\n\ndef _convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    \n    stoken = ['<s>'] + title + ['</s>','</s>'] + question + ['</s>','</s>'] + answer + ['</s>']\n\n    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n    input_masks = _get_masks(stoken, max_sequence_length)\n    input_segments = _get_segments(stoken, max_sequence_length)\n\n    return [input_ids, input_masks, input_segments]\n\ndef compute_input_arays(df, columns, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n\n        t, q, a = _trim_input(t, q, a, max_sequence_length)\n\n        ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [np.asarray(input_ids, dtype=np.int32), \n            np.asarray(input_masks, dtype=np.int32), \n            np.asarray(input_segments, dtype=np.int32)]\n\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:24:55.307285Z","iopub.execute_input":"2023-08-23T14:24:55.307713Z","iopub.status.idle":"2023-08-23T14:24:55.333218Z","shell.execute_reply.started":"2023-08-23T14:24:55.307681Z","shell.execute_reply":"2023-08-23T14:24:55.332009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:24:55.336348Z","iopub.execute_input":"2023-08-23T14:24:55.336626Z","iopub.status.idle":"2023-08-23T14:24:55.348210Z","shell.execute_reply.started":"2023-08-23T14:24:55.336603Z","shell.execute_reply":"2023-08-23T14:24:55.347037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TextDataset(torch.utils.data.TensorDataset):\n\n    def __init__(self, x_train, idxs, targets=None):\n        self.input_ids = x_train[0][idxs]\n        self.input_masks = x_train[1][idxs]\n        self.input_segments = x_train[2][idxs]\n        self.targets = targets[idxs] if targets is not None else np.zeros((x_train[0].shape[0], 30))\n\n    def __getitem__(self, idx):\n#         x_train = self.x_train[idx]\n        input_ids =  self.input_ids[idx]\n        input_masks = self.input_masks[idx]\n        input_segments = self.input_segments[idx]\n\n        target = self.targets[idx]\n\n        return input_ids, input_masks, input_segments, target\n\n    def __len__(self):\n        return len(self.input_ids)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:24:55.349625Z","iopub.execute_input":"2023-08-23T14:24:55.350091Z","iopub.status.idle":"2023-08-23T14:24:55.360184Z","shell.execute_reply.started":"2023-08-23T14:24:55.350060Z","shell.execute_reply":"2023-08-23T14:24:55.359253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer, get_cosine_with_hard_restarts_schedule_with_warmup","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:24:55.361756Z","iopub.execute_input":"2023-08-23T14:24:55.362214Z","iopub.status.idle":"2023-08-23T14:24:55.403643Z","shell.execute_reply.started":"2023-08-23T14:24:55.362183Z","shell.execute_reply":"2023-08-23T14:24:55.402447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_weights = 'roberta-base'\ntokenizer = RobertaTokenizer.from_pretrained(pretrained_weights)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:24:55.408298Z","iopub.execute_input":"2023-08-23T14:24:55.408568Z","iopub.status.idle":"2023-08-23T14:24:56.151266Z","shell.execute_reply.started":"2023-08-23T14:24:55.408544Z","shell.execute_reply":"2023-08-23T14:24:56.150202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\ndef decontracted(phrase):\n    phrase = re.sub(r\"(W|w)on(\\'|\\')t \", \"will not \", phrase)\n    phrase = re.sub(r\"(C|c)an(\\'|\\')t \", \"can not \", phrase)\n    phrase = re.sub(r\"(Y|y)(\\'|\\')all \", \"you all \", phrase)\n    phrase = re.sub(r\"(Y|y)a(\\'|\\')ll \", \"you all \", phrase)\n    phrase = re.sub(r\"(I|i)(\\'|\\')m \", \"i am \", phrase)\n    phrase = re.sub(r\"(A|a)isn(\\'|\\')t \", \"is not \", phrase)\n    phrase = re.sub(r\"n(\\'|\\')t \", \" not \", phrase)\n    phrase = re.sub(r\"(\\'|\\')re \", \" are \", phrase)\n    phrase = re.sub(r\"(\\'|\\')d \", \" would \", phrase)\n    phrase = re.sub(r\"(\\'|\\')ll \", \" will \", phrase)\n    phrase = re.sub(r\"(\\'|\\')t \", \" not \", phrase)\n    phrase = re.sub(r\"(\\'|\\')ve \", \" have \", phrase)\n    \n    return phrase\n\n\ndef clean_text(x):\n\n    x = str(x)\n    for punct in \"/-'\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n        x = x.replace(punct, '')\n    return x\n\ndef clean_numbers(x):\n\n    x = re.sub('[0-9]{5,}', '12345', x)\n    x = re.sub('[0-9]{4}', '1234', x)\n    x = re.sub('[0-9]{3}', '123', x)\n    x = re.sub('[0-9]{2}', '12', x)\n    return x\n\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]\n\nfrom tqdm import tqdm\ndef preprocess_text(text_data):\n    preprocessed_text = []\n    # tqdm is for printing the status bar\n    for sentance in tqdm(text_data):\n        sent = decontracted(sentance)\n        sent = clean_text(sentance)\n        sent = clean_numbers(sentance)\n        sent = sent.replace('\\\\r', ' ')\n        sent = sent.replace('\\\\n', ' ')\n        sent = sent.replace('\\\\\"', ' ')\n        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n        \n\n        sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n        preprocessed_text.append(sent.lower().strip())\n    return preprocessed_text\n\ntrain['question_title'] = preprocess_text(train['question_title'].values)\ntrain['question_body'] = preprocess_text(train['question_body'].values)\ntrain['answer'] = preprocess_text(train['answer'].values)\ntest['question_title'] = preprocess_text(test['question_title'].values)\ntest['question_body'] = preprocess_text(test['question_body'].values)\ntest['answer'] = preprocess_text(test['answer'].values)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:24:56.153031Z","iopub.execute_input":"2023-08-23T14:24:56.153632Z","iopub.status.idle":"2023-08-23T14:25:07.781923Z","shell.execute_reply.started":"2023-08-23T14:24:56.153593Z","shell.execute_reply":"2023-08-23T14:25:07.780913Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = compute_input_arays(train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\ny_train = compute_output_arrays(train, TARGET_COLUMNS)\nx_test = compute_input_arays(test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:25:07.787057Z","iopub.execute_input":"2023-08-23T14:25:07.789818Z","iopub.status.idle":"2023-08-23T14:25:30.390894Z","shell.execute_reply.started":"2023-08-23T14:25:07.789778Z","shell.execute_reply":"2023-08-23T14:25:30.389686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_config = RobertaConfig.from_pretrained(pretrained_weights) \nbert_config.num_labels = 29","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:25:30.392653Z","iopub.execute_input":"2023-08-23T14:25:30.393090Z","iopub.status.idle":"2023-08-23T14:25:30.446680Z","shell.execute_reply.started":"2023-08-23T14:25:30.393054Z","shell.execute_reply":"2023-08-23T14:25:30.445770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers.optimization import Adafactor, AdafactorSchedule\nnet = RobertaForSequenceClassification.from_pretrained(pretrained_weights, config=bert_config)\noptimizer = Adafactor(net.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\nlr_scheduler = AdafactorSchedule(optimizer)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:25:30.448100Z","iopub.execute_input":"2023-08-23T14:25:30.448485Z","iopub.status.idle":"2023-08-23T14:25:34.264087Z","shell.execute_reply.started":"2023-08-23T14:25:30.448451Z","shell.execute_reply":"2023-08-23T14:25:34.263169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nNFOLDS = 6\nBATCH_SIZE = 12\nEPOCHS = 8\nSEED = 7345\n\nseed_everything(SEED)\n\nmodel_list = list()\n\n\ny_oof = np.zeros((len(train), 29))\ntest_pred = np.zeros((len(test), 29))\n\ngradient_accumulation_steps = 1\nkf = KFold(n_splits=NFOLDS, shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(TextDataset(x_test, test.index),batch_size=BATCH_SIZE, shuffle=False)\n\ndevice=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nbest_acc=100\nfor i, (train_idx, valid_idx) in enumerate(kf.split(x_train[0])):\n    \n    \n    print(f'fold {i+1}')\n\n    ## loader\n    train_loader = torch.utils.data.DataLoader(TextDataset(x_train, train_idx, y_train),batch_size=BATCH_SIZE, shuffle=True)\n    val_loader = torch.utils.data.DataLoader(TextDataset(x_train, valid_idx, y_train),batch_size=BATCH_SIZE, shuffle=False)\n    \n\n    t_total = len(train_loader)//gradient_accumulation_steps*EPOCHS\n\n\n    net.to(device)\n    \n    loss_fn = torch.nn.BCEWithLogitsLoss()\n    #print(f'fold {i+1} optimizer')\n    \n    \n    for epoch in range(EPOCHS):  \n        #print(f'fold {i+1} epoch {epoch+1}')\n        start_time = time.time()\n        avg_loss = 0.0\n        net.train()\n        start_time1 = time.time()\n        end_time1 = time.time()\n        #print(f'fold {i+1} epoch {epoch+1} train:')\n        for step, data in enumerate(train_loader):\n            start1 = time.time()\n            #print(f'fold {i+1} epoch {epoch+1} train')\n            # get the inputs\n            \n            optimizer.zero_grad()\n            input_ids, input_masks, input_segments, labels = data\n            #net.train()\n            pred = net(input_ids = input_ids.long().to(device),\n                             labels = None,\n                             attention_mask = input_masks.to(device),\n                            )[0]\n            \n            \n            loss = loss_fn(pred, labels.to(device))\n        \n            avg_loss += loss.item()\n            loss = loss / gradient_accumulation_steps\n            loss.backward()\n\n            #if (step + 1) % 2 == 0:\n\n                # Calling the step function on an Optimizer makes an update to its parameters\n            optimizer.step()\n            lr_scheduler.step()\n            end1=time.time()\n            if (end1-start1)>=2:print(\"time boom!\")\n            \n        avg_val_loss = 0.0\n        #print(f'fold {i+1} epoch {epoch+1} val')\n        valid_preds = np.zeros((len(valid_idx), 29))\n        true_label = np.zeros((len(valid_idx), 29))\n        net.eval()\n        #print(f'fold {i+1} epoch {epoch+1} val')\n        for j,data in enumerate(val_loader):\n            \n            # get the inputs\n            input_ids, input_masks, input_segments, labels = data\n            with torch.no_grad():\n                pred = net(input_ids = input_ids.long().to(device),\n                                 labels = None,\n                                 attention_mask = input_masks.to(device),\n                                )[0]\n\n                loss_val = loss_fn(pred, labels.to(device))\n                avg_val_loss += loss_val.item()\n\n                pred = torch.sigmoid(pred)\n            valid_preds[j * BATCH_SIZE:(j+1) * BATCH_SIZE] = pred.cpu().detach().numpy()\n            true_label[j * BATCH_SIZE:(j+1) * BATCH_SIZE]  = labels\n            \n        \n        score = 0\n        for i in range(29):\n          s = np.nan_to_num(\n                    spearmanr(true_label[:, i], valid_preds[:, i]).correlation / 29)\n          score += s\n\n        \n        elapsed_time = time.time() - start_time \n        print('Epoch {}/{} \\t loss={:.4f}\\t val_loss={:.4f}\\t spearmanr={:.4f}\\t time={:.2f}s'.format(epoch+1, EPOCHS, avg_loss/len(train_loader),avg_val_loss/len(val_loader),score, elapsed_time))\n        val_acc=avg_val_loss/len(val_loader)\n        if val_acc < best_acc:\n                best_acc = val_acc\n                model_path = f\"model_epoch_{epoch + 1}_folder_{i+1}.pt\"\n                torch.save(net.state_dict(), model_path)\n                print('saving model with acc {:.3f}'.format(best_acc))\n        \n\n        \n    model_list.append(net)\n    y_oof[valid_idx] = valid_preds\n\n\n    result = list()\n    with torch.no_grad():\n        for data in test_loader:\n            input_ids, input_masks, input_segments, labels = data\n            y_pred = net(input_ids = input_ids.long().to(device),\n                                labels = None,\n                                attention_mask = input_masks.to(device),\n                            )[0]\n\n            y_pred = torch.sigmoid(y_pred)\n            result.extend(y_pred.cpu().detach().numpy())\n            \n    test_pred += np.array(result)/NFOLDS\n\n\n        \n","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:25:34.265803Z","iopub.execute_input":"2023-08-23T14:25:34.266231Z","iopub.status.idle":"2023-08-23T14:39:19.261827Z","shell.execute_reply.started":"2023-08-23T14:25:34.266193Z","shell.execute_reply":"2023-08-23T14:39:19.260759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n    \ndef postprocessing(oof_df):\n   \n    scaler = MinMaxScaler()\n    \n    # type 1 column [0, 0.333333, 0.5, 0.666667, 1]\n    # type 2 column [0, 0.333333, 0.666667]\n    # type 3 column [0.333333, 0.444444, 0.5, 0.555556, 0.666667, 0.777778, 0.8333333, 0.888889, 1]\n    # type 4 column [0.200000, 0.266667, 0.300000, 0.333333, 0.400000, \\\n    # 0.466667, 0.5, 0.533333, 0.600000, 0.666667, 0.700000, \\\n    # 0.733333, 0.800000, 0.866667, 0.900000, 0.933333, 1]\n    \n    # comment some columns based on oof result\n    \n    ################################################# handle type 1 columns\n    type_one_column_list = [\n       'question_conversational', \\\n       'question_has_commonly_accepted_answer', \\\n       'question_not_really_a_question', \\\n       'question_type_choice', \\\n       'question_type_compare', \\\n       'question_type_consequence', \\\n       'question_type_definition', \\\n       'question_type_entity', \\\n       'question_type_instructions', \n    ]\n    \n    oof_df[type_one_column_list] = scaler.fit_transform(oof_df[type_one_column_list])\n    \n    tmp = oof_df.copy(deep=True)\n    \n    for column in type_one_column_list:\n        \n        oof_df.loc[tmp[column] <= 0.16667, column] = 0\n        oof_df.loc[(tmp[column] > 0.16667) & (tmp[column] <= 0.41667), column] = 0.333333\n        oof_df.loc[(tmp[column] > 0.41667) & (tmp[column] <= 0.58333), column] = 0.500000\n        oof_df.loc[(tmp[column] > 0.58333) & (tmp[column] <= 0.73333), column] = 0.666667\n        oof_df.loc[(tmp[column] > 0.73333), column] = 1\n    \n    \n    \n    ################################################# handle type 2 columns      \n#     type_two_column_list = [\n#         'question_type_spelling'\n#     ]\n    \n#     for column in type_two_column_list:\n#         if sum(tmp[column] > 0.15)>0:\n#             oof_df.loc[tmp[column] <= 0.15, column] = 0\n#             oof_df.loc[(tmp[column] > 0.15) & (tmp[column] <= 0.45), column] = 0.333333\n#             oof_df.loc[(tmp[column] > 0.45), column] = 0.666667\n#         else:\n#             t1 = max(int(len(tmp[column])*0.0013),2)\n#             t2 = max(int(len(tmp[column])*0.0008),1)\n#             thred1 = sorted(list(tmp[column]))[-t1]\n#             thred2 = sorted(list(tmp[column]))[-t2]\n#             oof_df.loc[tmp[column] <= thred1, column] = 0\n#             oof_df.loc[(tmp[column] > thred1) & (tmp[column] <= thred2), column] = 0.333333\n#             oof_df.loc[(tmp[column] > thred2), column] = 0.666667\n    \n    \n    \n    ################################################# handle type 3 columns      \n    type_three_column_list = [\n       'question_interestingness_self', \n    ]\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    oof_df[type_three_column_list] = scaler.fit_transform(oof_df[type_three_column_list])\n    tmp[type_three_column_list] = scaler.fit_transform(tmp[type_three_column_list])\n    \n    for column in type_three_column_list:\n        oof_df.loc[tmp[column] <= 0.385, column] = 0.333333\n        oof_df.loc[(tmp[column] > 0.385) & (tmp[column] <= 0.47), column] = 0.444444\n        oof_df.loc[(tmp[column] > 0.47) & (tmp[column] <= 0.525), column] = 0.5\n        oof_df.loc[(tmp[column] > 0.525) & (tmp[column] <= 0.605), column] = 0.555556\n        oof_df.loc[(tmp[column] > 0.605) & (tmp[column] <= 0.715), column] = 0.666667\n        oof_df.loc[(tmp[column] > 0.715) & (tmp[column] <= 0.8), column] = 0.833333\n        oof_df.loc[(tmp[column] > 0.8) & (tmp[column] <= 0.94), column] = 0.888889\n        oof_df.loc[(tmp[column] > 0.94), column] = 1\n        \n        \n        \n    ################################################# handle type 4 columns      \n    type_four_column_list = [\n        'answer_satisfaction'\n    ]\n    scaler = MinMaxScaler(feature_range=(0.2, 1))\n    oof_df[type_four_column_list] = scaler.fit_transform(oof_df[type_four_column_list])\n    tmp[type_four_column_list] = scaler.fit_transform(tmp[type_four_column_list])\n    \n    for column in type_four_column_list:\n        \n        oof_df.loc[tmp[column] <= 0.233, column] = 0.200000\n        oof_df.loc[(tmp[column] > 0.233) & (tmp[column] <= 0.283), column] = 0.266667\n        oof_df.loc[(tmp[column] > 0.283) & (tmp[column] <= 0.315), column] = 0.300000\n        oof_df.loc[(tmp[column] > 0.315) & (tmp[column] <= 0.365), column] = 0.333333\n        oof_df.loc[(tmp[column] > 0.365) & (tmp[column] <= 0.433), column] = 0.400000\n        oof_df.loc[(tmp[column] > 0.433) & (tmp[column] <= 0.483), column] = 0.466667\n        oof_df.loc[(tmp[column] > 0.483) & (tmp[column] <= 0.517), column] = 0.500000\n        oof_df.loc[(tmp[column] > 0.517) & (tmp[column] <= 0.567), column] = 0.533333\n        oof_df.loc[(tmp[column] > 0.567) & (tmp[column] <= 0.633), column] = 0.600000\n        oof_df.loc[(tmp[column] > 0.633) & (tmp[column] <= 0.683), column] = 0.666667\n        oof_df.loc[(tmp[column] > 0.683) & (tmp[column] <= 0.715), column] = 0.700000\n        oof_df.loc[(tmp[column] > 0.715) & (tmp[column] <= 0.767), column] = 0.733333\n        oof_df.loc[(tmp[column] > 0.767) & (tmp[column] <= 0.833), column] = 0.800000\n        oof_df.loc[(tmp[column] > 0.883) & (tmp[column] <= 0.915), column] = 0.900000\n        oof_df.loc[(tmp[column] > 0.915) & (tmp[column] <= 0.967), column] = 0.933333\n        oof_df.loc[(tmp[column] > 0.967), column] = 1\n    \n    \n    ################################################# round to i / 90 (i from 0 to 90)\n    oof_values = oof_df[TARGET_COLUMNS].values\n    DEGREE = len(oof_df)//45*9\n#     if degree:\n#         DEGREE = degree\n#     DEGREE = 90\n    oof_values = np.around(oof_values * DEGREE) / DEGREE  ### 90 To be changed\n    oof_df[TARGET_COLUMNS] = oof_values\n    \n    return oof_df","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:39:19.263522Z","iopub.execute_input":"2023-08-23T14:39:19.264034Z","iopub.status.idle":"2023-08-23T14:39:19.291323Z","shell.execute_reply.started":"2023-08-23T14:39:19.263969Z","shell.execute_reply":"2023-08-23T14:39:19.290264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.loc[:, TARGET_COLUMNS] = test_pred\nsample_submission = postprocessing(sample_submission)\nsample_submission[ sample_submission[TARGET_COLUMNS] > 1.0] = 1.0\nsample_submission['question_type_spelling'] = spelling","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:40:21.203407Z","iopub.execute_input":"2023-08-23T14:40:21.203799Z","iopub.status.idle":"2023-08-23T14:40:21.287910Z","shell.execute_reply.started":"2023-08-23T14:40:21.203765Z","shell.execute_reply":"2023-08-23T14:40:21.286889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nsample_submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T14:40:25.032500Z","iopub.execute_input":"2023-08-23T14:40:25.033511Z","iopub.status.idle":"2023-08-23T14:40:25.069120Z","shell.execute_reply.started":"2023-08-23T14:40:25.033468Z","shell.execute_reply":"2023-08-23T14:40:25.068025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}